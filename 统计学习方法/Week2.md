# 感知机

二类分类的线性分类模型;
特征向量->$\plusmn1$

## 学习策略

### 线性可分

如果数据集的点可被超平面按照负实例和正实例完美分成廉租，则称其线性可分。

### 损失函数

如果数据集线性可分，则定义感知机$sign(w\cdot x+b)$学习的损失函数为

$$
L(w,b)=-\sum_{x_i\in M} y_i(w\cdot x_i+b)
$$

其中M为误分类点的集合。

## 学习算法

最优化问题的算法：给定训练集
$$
T=\{(x_i,y_i)|i=1,2,\cdots ,n\}
$$

 求参数$w,b$使得

$$
L(w,b)=-\sum_{x_i\in M} y_i(w\cdot x_i+b)
$$

### 原始形式

采用随机梯度下降法：任意选取超平面$w_0,b_0$然后用梯度下降法不断极小化目标函数；及消化过程不是一次使M中所有误分类点梯度下降，而是从随机选七一个误分类点使其梯度下降。

$$
\nabla_w L=-\sum_{x_i\in M}y_ix_i\\
\nabla_b L=-\sum_{x_i\in M}y_i
$$

则每次随机选取误分类点对$w,b$进行迭代更新

1. 选定初值
2. 选定数据
3. 如果$y_i(w\cdot x_i+b)\le 0$则施迭代
    $$
    w\leftarrow w+\eta y_ix_i\\
    b\leftarrow b+\eta y_i
    $$
4. 转至2，直到训练集中无误分类点

$\eta$为步长，又称学习率。

线性可分数据集，对此算法原始形式收敛。；如果不收敛，则迭代效果会发生震荡。

### 对偶形式

设初始的$w_0=0,b_0=0$则
$$
w=\sum_{i=1}^N\alpha_iy_ix_i\\
b=\sum_{i=1}^N\alpha_iy_i
$$
其中$\alpha_i\ge 0,\alpha_i=n_i\eta$

1. 选定初值(0,0)
2. 选定数据
3. 如果$y_i(\sum_{j=1}^N\alpha_jy_jx_j\cdot x_i+b)\le 0$则施迭代
    $$
    \alpha_i\leftarrow \alpha_i+\eta\\
    b\leftarrow b+\eta y_i
    $$
4. 转至2，直到训练集中无误分类点

此算法在，可以将训练集中的实例见数据的内积预先计算处理，称为Gram矩阵
$$
\boldsymbol{G}=[x_i\cdot y_i]_{N\times N}
$$

## 习题

### 2.1

异或的结果线性不可分。

考虑习题2.3中的定理，结论是显然的。

### 2.2

。。。

### 2.3

凸壳==凸包

必要性：采用反证法证明。

假设两个凸包相交，交集为空间S，且该样本集线性可被超平面P分割。

考虑到S同时包含正实例和负实例，则S也被P分割。

设S中，负实例的点集合为$S_1$，那么根据S被P分割，$S_2=S-S_1$是一个连续的空间，且里边只有正实例点。

那么如果：

1. $S_1$中的点和其它负实例点都位于P的同一侧，那么正负实例的凸包不相交，矛盾。
2. $S_1$中的点和其它负实例点都位于P的异侧，则整个空间未被P分割，矛盾。

充分性：

显然，如果样本集线性可分，那么正负实例的凸包分别位于分割超平面P的两侧，必然互不相交。

